{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP9rhmOuqtTiZmsWUXJWXhR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIPZuILJWsKI","executionInfo":{"status":"ok","timestamp":1728740039311,"user_tz":-330,"elapsed":4010,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}},"outputId":"d8d7e27f-83cd-4105-f7ea-d42933f191b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install gym torch numpy matplotlib"]},{"cell_type":"code","source":["import gym\n","import math\n","import random\n","import numpy as np\n","from collections import deque, namedtuple\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","ENV_NAME = 'CartPole-v1'\n","BATCH_SIZE = 64\n","GAMMA = 0.99\n","EPS_START = 1.0\n","EPS_END = 0.01\n","EPS_DECAY = 500\n","TARGET_UPDATE = 10\n","MEMORY_SIZE = 10000\n","LR = 1e-3\n","NUM_EPISODES = 500\n","PER_EPSILON = 1e-6\n","ALPHA = 0.6  # Importance-sampling factor\n","\n","# Replay Memory\n","Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n","\n","class PrioritizedReplayMemory:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.pos = 0\n","        self.priorities = []\n","\n","    def push(self, *args):\n","        max_priority = max(self.priorities) if self.priorities else 1.0\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(Transition(*args))\n","            self.priorities.append(max_priority)\n","        else:\n","            self.memory[self.pos] = Transition(*args)\n","            self.priorities[self.pos] = max_priority\n","            self.pos = (self.pos + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        scaled_priorities = np.array(self.priorities) ** ALPHA\n","        sampling_probabilities = scaled_priorities / np.sum(scaled_priorities)\n","        indices = np.random.choice(len(self.memory), batch_size, p=sampling_probabilities)\n","        samples = [self.memory[i] for i in indices]\n","        importance = (len(self.memory) * sampling_probabilities[indices]) ** (-1)\n","        return samples, indices, importance\n","\n","    def update_priorities(self, indices, priorities):\n","        for idx, priority in zip(indices, priorities.flatten()):  # Ensure it's iterable\n","            self.priorities[idx] = priority\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","# Dueling DQN Network\n","class DuelingDQN(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DuelingDQN, self).__init__()\n","        self.feature = nn.Sequential(\n","            nn.Linear(state_size, 128),\n","            nn.ReLU(),\n","        )\n","\n","        # Value Stream\n","        self.value_stream = nn.Sequential(\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","        # Advantage Stream\n","        self.advantage_stream = nn.Sequential(\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, action_size)\n","        )\n","\n","    def forward(self, x):\n","        x = self.feature(x)\n","        value = self.value_stream(x)\n","        advantage = self.advantage_stream(x)\n","        # Combine them to get Q-values\n","        qvals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n","        return qvals\n","\n","# Epsilon Greedy Policy\n","def select_action(state, policy_net, steps_done, n_actions):\n","    epsilon = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps_done / EPS_DECAY)\n","    sample = random.random()\n","    if sample > epsilon:\n","        with torch.no_grad():\n","            return policy_net(state).max(1)[1].view(1, 1)\n","    else:\n","        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n","\n","# Plotting function\n","def plot_durations(episode_durations, avg_window=100):\n","    plt.figure(1)\n","    plt.clf()\n","    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n","    plt.title('Training DQN with Prioritized Experience Replay')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Duration')\n","    plt.plot(durations_t.numpy())\n","    # Compute and plot the average\n","    if len(durations_t) >= avg_window:\n","        means = durations_t.unfold(0, avg_window, 1).mean(1).flatten()\n","        means = torch.cat((torch.zeros(avg_window-1), means))\n","        plt.plot(means.numpy())\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","def main():\n","    env = gym.make(ENV_NAME)\n","    n_actions = env.action_space.n\n","    state_size = env.observation_space.shape[0]\n","\n","    policy_net = DuelingDQN(state_size, n_actions).to(device)\n","    target_net = DuelingDQN(state_size, n_actions).to(device)\n","    target_net.load_state_dict(policy_net.state_dict())\n","    target_net.eval()\n","\n","    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n","    memory = PrioritizedReplayMemory(MEMORY_SIZE)\n","\n","    steps_done = 0\n","    episode_durations = []\n","\n","    for episode in range(NUM_EPISODES):\n","        state = env.reset()\n","        state = torch.tensor([state], device=device, dtype=torch.float)\n","        total_reward = 0\n","        for t in range(1, 10000):  # Don't infinite loop\n","            action = select_action(state, policy_net, steps_done, n_actions)\n","            steps_done += 1\n","            next_state, reward, done, _ = env.step(action.item())\n","            total_reward += reward\n","            reward = torch.tensor([reward], device=device, dtype=torch.float)\n","            next_state = torch.tensor([next_state], device=device, dtype=torch.float)\n","            done_flag = torch.tensor([done], device=device, dtype=torch.float)\n","\n","            # Store experience with initial priority\n","            memory.push(state, action, reward, next_state, done_flag)\n","\n","            state = next_state\n","\n","            # Perform optimization\n","            if len(memory) >= BATCH_SIZE:\n","                transitions, indices, importance = memory.sample(BATCH_SIZE)\n","                batch = Transition(*zip(*transitions))\n","\n","                # Convert to tensors\n","                state_batch = torch.cat(batch.state)\n","                action_batch = torch.cat(batch.action)\n","                reward_batch = torch.cat(batch.reward)\n","                next_state_batch = torch.cat(batch.next_state)\n","                done_batch = torch.cat(batch.done)\n","\n","                # Compute Q(s_t, a)\n","                state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","                # Compute V(s_{t+1}) for all next states.\n","                with torch.no_grad():\n","                    next_state_values = target_net(next_state_batch).max(1)[0].unsqueeze(1)\n","                    # Compute the expected Q values\n","                    expected_state_action_values = reward_batch.unsqueeze(1) + (GAMMA * next_state_values * (1 - done_batch.unsqueeze(1)))\n","\n","                # Compute loss\n","                loss = F.mse_loss(state_action_values, expected_state_action_values)\n","\n","                # Update priorities\n","                new_priorities = (loss.detach().cpu().numpy() + PER_EPSILON).flatten()  # Ensure it's 1D\n","                memory.update_priorities(indices, new_priorities)\n","\n","                # Optimize the model\n","                optimizer.zero_grad()\n","                loss.backward()\n","                # Clip gradients to prevent explosion\n","                for param in policy_net.parameters():\n","                    param.grad.data.clamp_(-1, 1)\n","                optimizer.step()\n","\n","            if done:\n","                episode_durations.append(total_reward)\n","                if episode % TARGET_UPDATE == 0:\n","                    target_net.load_state_dict(policy_net.state_dict())\n","                print(f\"Episode {episode}: Total Reward: {total_reward}\")\n","                break\n","\n","        # Optionally plot\n","        if episode % 10 == 0:\n","            plot_durations(episode_durations)\n","\n","    print('Training complete')\n","    env.close()\n","    # Show final plot\n","    plt.ioff()\n","    plot_durations(episode_durations)\n","    plt.show()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FGHQyTgh-Ut9awsgBh00BaMXulyO6vWX"},"id":"qRSfq57oXKlL","executionInfo":{"status":"ok","timestamp":1728740735947,"user_tz":-330,"elapsed":690722,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}},"outputId":"e2e6e76b-0fd8-4acc-bba4-001838ee9d3e"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}